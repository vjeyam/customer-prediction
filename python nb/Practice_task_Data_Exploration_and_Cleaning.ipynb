{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKU2obxkKH8c"
   },
   "source": [
    "# Welcome the practice notebook \n",
    "--- \n",
    "The objective of this practice is to provide a hands-on experience in data exploration and cleaning. Please follow the instructions provided in the  notebook to complete the practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1xXctOSKf36"
   },
   "source": [
    "Installing pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLd6XoJtJZP4",
    "outputId": "d42992e3-86e1-4510-ba37-2e0e25fe4590"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gA6GqvzKpR6"
   },
   "source": [
    "Importing the needed modules and creating the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "c-YsrJbjKptK",
    "outputId": "c316ee30-bbcc-4446-b5d9-f5425451f525"
   },
   "outputs": [],
   "source": [
    "# importing spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# data visualization modules \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px \n",
    "\n",
    "# pandas module \n",
    "import pandas as pd\n",
    "\n",
    "# pyspark SQL functions \n",
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# creating the spark session\n",
    "spark = SparkSession.builder.appName(\"Customer_Churn_Prediction\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn6fCpN5K5jb"
   },
   "source": [
    "Loading the `parctice-dataset.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EKyTAZYKyTr",
    "outputId": "f0a99f3e-70e8-434e-920d-54d3341d2f8d"
   },
   "outputs": [],
   "source": [
    "data = spark.read.format('csv').option('header',True).option('inferSchema',True).load('practice_dataset.csv')\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5CQl_LEVHW3"
   },
   "source": [
    "How many rows and columns do we have in the dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLhqklGYVDsx",
    "outputId": "5004e93e-37f2-440e-e0ad-13a2c4eff354"
   },
   "outputs": [],
   "source": [
    "# write your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU590GfcViTP"
   },
   "source": [
    "Create a histogram matrix to analyse the distributions of the numerical columns. \n",
    "\n",
    "- *Hint 1: List of the numerical columns = `[\"KPI1\",\"KPI2\",\"KPI3\",\"KPI4\"]`* <br>\n",
    "- *Hint 2: To create the histograms, first you will need to convert the pyspark dataframe into a pandas dataframe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "0JtVJ2SGVrV3",
    "outputId": "739b727b-483d-4ea4-a951-61e03af1aa58"
   },
   "outputs": [],
   "source": [
    "# write your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3RKi7OHWJWP"
   },
   "source": [
    "Observe the distributions of the numerical columns in the histograms generated in the previous part.\n",
    "- Find the columns with outilers\n",
    "- Remove the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0DVLkyrWTF_",
    "outputId": "cec525cc-6157-4458-ee5b-d8691493afe7"
   },
   "outputs": [],
   "source": [
    "# write your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B59cm1jwXBuX"
   },
   "source": [
    "Find the missing values in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-QiT3txWyj3",
    "outputId": "ed0d1139-739e-4c84-eaae-64629d48f174"
   },
   "outputs": [],
   "source": [
    "# write your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMOG53waYjoI"
   },
   "source": [
    "Use **Mean Imputer** to fill the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYK7AF1aYpAW",
    "outputId": "f8b865be-8220-4020-9261-984742394eae"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "# write your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whcp-_jbbmQ-"
   },
   "source": [
    "Again, find the missing values in the dataset to make sure no missing value exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUhsVtYQa5bm",
    "outputId": "3138e078-ecfd-4703-9769-6b790b950381"
   },
   "outputs": [],
   "source": [
    "# write your code here "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
